

import visdom
vis = visdom.Visdom()

class env_stock():
    def __init__(self):
        stock_code = '005930'
        start_date = '2010-03-01'
        end_date = '2015-03-04'

        chart_data['data']= pd.to_datetime(chart_data.date).astype(np.int64)/1000000
        data = torch.from_numpy(chart_data.values)

        self.data = torch.stack([data[:,0],data[:,4],data[:,5]],dim=1).float()

        self.data = self.data - self.data.mean(dim=0)
        self.data = self.data/self.data.std(0)

        self.count_max= self.data.size(0)

    def step(self,action):
        #continuouse action space
        quantize = 100
        a_t = round((self.prev_action - action)*quantize)
        r_t = d_t= 0

        if a_t >=0:
            self.pocket += self.data[self.count,1]*a_t
        else:
            self.pocket += self.data[self.count,1]*a_t

        self.prev_action = action

        if self.count+1 == self.count_max:
            self.pocket += self.data[self.count,1]*(self.prev_action*quantize)
            d_t = 1
            r_t = self.pocket 

        else:
            self.count +=1

        return self.data[self.count].view(1,-1),r_t ,d_t
        pass


env = env_stock()
env.reset()

dev = torch.device('cpu')
print(dev)
from DDPG_network import Agent

agent = Agent(nb_states=3,nb_action=1,mem_size=10000,dev=dev)
agent.train()

for episode in range(1000):
    testset = True if (episode+5)%100 == 0 else False
    ite = 0
    test_epi = 0
    train_epi = 0
    max_v_l = -9999
    max_p_l = -9999
    mem = []
    s_t = env.reset()
    total_reward = 0
    eps = 0 if testset else 0.3

    for T in range(10000):
        a_t = agent.get_action(a_t.item())

